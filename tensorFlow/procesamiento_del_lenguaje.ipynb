{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KiCCBsIkMHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d9879d7-c2fc-42d1-8d12-85478dc32e70"
      },
      "source": [
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
        "  bag = {}  # stores all of the encodings and their frequency\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      encoding = vocab[word]  # get encoding from vocab\n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding = word_encoding\n",
        "      word_encoding += 1\n",
        "    \n",
        "    if encoding in bag:\n",
        "      bag[encoding] += 1\n",
        "    else:\n",
        "      bag[encoding] = 1\n",
        "  \n",
        "  return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miYshfvzmJ0H"
      },
      "source": [
        "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
        "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
        "\n",
        "pos_bag = bag_of_words(positive_review)\n",
        "neg_bag = bag_of_words(negative_review)\n",
        "\n",
        "print(\"Positive:\", pos_bag)\n",
        "print(\"Negative:\", neg_bag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################"
      ],
      "metadata": {
        "id": "lOEVV76oKfzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKY4y_tjnUEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f706144-c188-4b79-b3b3-42f7fd460cac"
      },
      "source": [
        "vocab = {}  \n",
        "word_encoding = 1\n",
        "def one_hot_encoding(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \") \n",
        "  encoding = []  \n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      code = vocab[word]  \n",
        "      encoding.append(code) \n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding.append(word_encoding)\n",
        "      word_encoding += 1\n",
        "  \n",
        "  return encoding\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "encoding = one_hot_encoding(text)\n",
        "print(encoding)\n",
        "print(vocab)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 1, 4, 8, 9, 2, 2, 4, 3, 3]\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S-GNjotn-Br"
      },
      "source": [
        "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n",
        "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n",
        "\n",
        "pos_encode = one_hot_encoding(positive_review)\n",
        "neg_encode = one_hot_encoding(negative_review)\n",
        "\n",
        "print(\"Positive:\", pos_encode)\n",
        "print(\"Negative:\", neg_encode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################"
      ],
      "metadata": {
        "id": "Pc2ZYvgwLkLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fju7i1FKrK_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a704df2-c19b-4fee-ffe2-0a061f416b85"
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdRcVIhtRGlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d43d2e-7611-43ca-a45d-13c83353fbc2"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n4oovOMRnP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "730ddd56-a4e4-467c-fba2-da8523402b52"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHUxQVl7Rt10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f7ce0b-db18-4268-8e92-2947d98eee2d"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7AZNI7aRz6y"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i5kvmX_SLW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb84c1a-6c58-45b2-cfea-1a13b000f44d"
      },
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af52YChSW5hX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b332c1e2-555a-402c-c5a1-dbc9cdb93678"
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBkXz9fjUQHW"
      },
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi0xaPB_VOJl"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03zKVHTvV0Km"
      },
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p_y2YmgWbnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f277fc32-d1b4-41a2-ae10-95cb47d75d39"
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRsKcjhXXuoD"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v_P2dEic4qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da41184c-0a0b-455d-8325-caa07b04a044"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdvEqlwc6_q0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0002ea8-d6ba-4c35-8daa-d1dfec4573f5"
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQS5KXwi7_NX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa529758-8a7c-49b3-e109-41e2d4ccef11"
      },
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-1.29965274e-03  8.54710932e-04  1.84649730e-03 ... -4.50863212e-04\n",
            "   -7.36998161e-04  1.90412602e-03]\n",
            "  [-3.36400419e-03  1.37687335e-03  4.03792411e-03 ...  1.86994066e-03\n",
            "    1.93854386e-04  5.35858935e-03]\n",
            "  [ 5.71653480e-04  5.34666283e-03  5.42639615e-03 ... -5.22005954e-04\n",
            "   -5.75351180e-04  5.87358885e-03]\n",
            "  ...\n",
            "  [ 2.31691916e-03 -1.22586056e-03  7.70068029e-04 ... -1.88524497e-03\n",
            "    2.43506534e-03  7.00462237e-03]\n",
            "  [ 1.96251087e-03 -3.79738631e-04  3.37652164e-03 ...  1.00943580e-04\n",
            "    2.54759798e-03  9.44420323e-03]\n",
            "  [ 1.16185169e-04  5.27139986e-03  7.19259959e-04 ... -4.10328433e-03\n",
            "   -2.56430311e-03  5.72836772e-03]]\n",
            "\n",
            " [[ 1.94709492e-03  4.04444523e-03 -2.02545919e-03 ... -1.18767819e-03\n",
            "   -6.86408300e-03 -3.37454723e-03]\n",
            "  [ 2.01021685e-04  3.47606768e-03  1.44654245e-04 ... -2.48834467e-03\n",
            "   -5.68174943e-03 -5.15185820e-04]\n",
            "  [-2.20450992e-03  3.08946380e-03  2.64029717e-03 ... -3.93130176e-04\n",
            "   -3.23315547e-03  3.47201014e-03]\n",
            "  ...\n",
            "  [-4.44218703e-03  8.81665293e-03  1.97829143e-03 ... -5.86954411e-03\n",
            "   -6.68155728e-03  8.50765221e-03]\n",
            "  [-6.17621094e-03  8.29980429e-03  1.80748024e-03 ... -4.68450738e-03\n",
            "   -8.46612174e-03  5.27891982e-03]\n",
            "  [-7.03717722e-03  8.03153217e-03  1.94410374e-03 ... -4.32283059e-03\n",
            "   -9.82202962e-03  2.97784852e-03]]\n",
            "\n",
            " [[-3.54752271e-03  2.49553495e-03  1.94738721e-04 ...  7.10423687e-04\n",
            "   -2.71501625e-03 -1.66924170e-03]\n",
            "  [-3.84652521e-03  1.98556948e-03  2.61869165e-03 ...  2.42485339e-03\n",
            "   -1.11083360e-03  3.06269526e-03]\n",
            "  [-1.53506873e-03  2.99561396e-03  1.23742386e-03 ... -2.75125634e-03\n",
            "   -1.32794178e-03  1.14347069e-02]\n",
            "  ...\n",
            "  [ 1.43921713e-03  3.69369192e-03  1.40020372e-02 ...  1.46898336e-03\n",
            "    7.75154447e-03 -3.86295933e-03]\n",
            "  [ 1.35983049e-03  6.16580155e-03  1.19249057e-02 ...  1.73149677e-03\n",
            "    1.03469789e-02  1.50890229e-03]\n",
            "  [ 7.59096351e-03  6.99311681e-03  1.20974509e-02 ...  6.02517393e-05\n",
            "    1.00243632e-02 -7.76903704e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 6.88519649e-05  5.70194516e-03  3.94023117e-03 ... -2.51844712e-03\n",
            "   -4.33944026e-03  6.24765933e-04]\n",
            "  [-1.26503385e-03  4.22703428e-03  5.27735567e-03 ...  4.96561406e-04\n",
            "   -2.13523535e-03  3.60723725e-03]\n",
            "  [-2.28797132e-03  7.79652875e-03  5.31932758e-03 ... -7.09044049e-03\n",
            "    4.13844595e-04  2.69826921e-03]\n",
            "  ...\n",
            "  [-1.14000018e-03  3.20732268e-03  6.69141207e-03 ... -1.03400173e-02\n",
            "   -3.22107435e-03  3.22943414e-03]\n",
            "  [-3.43118003e-03  6.62188651e-03  3.03453649e-03 ... -1.12272520e-02\n",
            "   -5.59248775e-03  7.13628717e-04]\n",
            "  [-4.08465369e-03  2.67802412e-03  5.15223946e-03 ... -8.49034078e-03\n",
            "   -2.33751198e-05  2.56854342e-04]]\n",
            "\n",
            " [[-5.06199198e-04 -3.39161925e-04 -1.93454593e-03 ... -2.69636977e-04\n",
            "    4.53894027e-04  5.65795414e-03]\n",
            "  [-5.89704607e-04  1.46585400e-03 -2.17100373e-03 ... -4.45508305e-03\n",
            "    7.12660840e-05  1.28148356e-02]\n",
            "  [ 4.74621169e-03  3.72938160e-03  2.19594804e-03 ... -5.70957828e-03\n",
            "    8.23277864e-04 -4.28333413e-04]\n",
            "  ...\n",
            "  [ 3.68841866e-04  2.99257692e-04 -2.11059907e-03 ... -5.27487323e-03\n",
            "    9.92232701e-04 -6.34796871e-03]\n",
            "  [-4.63075237e-04 -6.28195377e-03 -5.48166130e-03 ... -1.08965505e-02\n",
            "   -3.99964955e-03 -1.52801303e-02]\n",
            "  [-1.62311806e-03 -1.10006556e-02 -7.10017327e-03 ... -9.84136201e-03\n",
            "   -1.83514552e-04 -1.62621625e-02]]\n",
            "\n",
            " [[ 9.40138008e-04  2.04828940e-03  1.29185989e-03 ...  1.32267084e-03\n",
            "    4.21361346e-03  4.44959709e-03]\n",
            "  [ 5.01881540e-03  3.58731998e-03  4.07177024e-03 ... -1.64889987e-03\n",
            "    4.24129609e-03 -6.73073949e-03]\n",
            "  [ 2.75791157e-04  7.13667274e-03  2.49752542e-04 ... -4.10606107e-03\n",
            "   -1.76654197e-04 -7.29840016e-03]\n",
            "  ...\n",
            "  [-3.49982525e-03 -7.33119715e-03  4.39819694e-03 ... -8.33997410e-03\n",
            "    1.25930319e-03 -4.54611564e-03]\n",
            "  [-5.54028898e-03 -7.17185810e-03  1.93490926e-03 ... -4.79933619e-03\n",
            "   -4.72192187e-03 -5.71148423e-03]\n",
            "  [-6.85972907e-03 -6.90971687e-03  3.63196433e-03 ... -1.10467081e-03\n",
            "   -1.75568066e-03  1.24753662e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA1Zhop28V9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a3a211-64b2-4eab-c198-e3825691cd7a"
      },
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00129965  0.00085471  0.0018465  ... -0.00045086 -0.000737\n",
            "   0.00190413]\n",
            " [-0.003364    0.00137687  0.00403792 ...  0.00186994  0.00019385\n",
            "   0.00535859]\n",
            " [ 0.00057165  0.00534666  0.0054264  ... -0.00052201 -0.00057535\n",
            "   0.00587359]\n",
            " ...\n",
            " [ 0.00231692 -0.00122586  0.00077007 ... -0.00188524  0.00243507\n",
            "   0.00700462]\n",
            " [ 0.00196251 -0.00037974  0.00337652 ...  0.00010094  0.0025476\n",
            "   0.0094442 ]\n",
            " [ 0.00011619  0.0052714   0.00071926 ... -0.00410328 -0.0025643\n",
            "   0.00572837]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbIoe7Ei8q3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38a7555a-6253-4c4d-9414-2b31cac3c101"
      },
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-1.2996527e-03  8.5471093e-04  1.8464973e-03 -4.3031396e-03\n",
            " -2.1231712e-03  1.9118026e-03 -7.1855080e-03  1.2717426e-03\n",
            "  3.7332836e-03 -9.2858286e-04  6.4186738e-03 -6.0038624e-04\n",
            "  1.0941646e-03  3.2794923e-03 -2.7789781e-03 -1.5806772e-03\n",
            " -2.6015349e-04 -2.3917772e-03  3.8676162e-04 -6.4484100e-04\n",
            "  2.9694808e-03  5.3490154e-03 -1.8174126e-03 -2.2812355e-03\n",
            "  1.5996111e-03  1.2269681e-03 -5.3743911e-03  2.5947588e-03\n",
            "  5.7861041e-03  1.9185508e-03 -3.5521371e-04 -2.9481405e-03\n",
            "  1.3121362e-03 -1.0299094e-03 -4.7713662e-03  2.8356737e-03\n",
            " -2.0860715e-03 -3.0937770e-03  1.5325267e-03  1.0706846e-03\n",
            "  3.1268725e-04 -2.2802476e-03  2.7441187e-04 -9.4257062e-05\n",
            "  6.9810636e-03 -1.4984476e-03  8.2008686e-04 -4.5461296e-03\n",
            "  2.0671240e-04  2.9623492e-03 -5.0345305e-03 -2.2395214e-03\n",
            "  4.6349615e-03  2.8635177e-03 -9.4356313e-03  2.7669210e-03\n",
            " -8.0041576e-04 -1.9811187e-03  2.7479073e-03 -1.2276325e-03\n",
            " -1.2400699e-03 -6.7995093e-04 -4.5086321e-04 -7.3699816e-04\n",
            "  1.9041260e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlEYM1H995gR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a553f059-d119-454f-a1cd-afead50986f3"
      },
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"NUA U&OaS:nerlDCsFPd?OPl AJk!VgHHU:T&aqSKucl,-pvJ'wSC3Zz 3cw;?yXZKZ:y ZRMYI-T$iwv,incfqAdpQodVUwNqNi\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOw23fWq9D9O"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g6o7zA_hAiS"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7aMushYjSpy"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4PAgrwMjZ4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6f6137-fbd2-44c5-a6a0-1c0753972552"
      },
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 1491s 9s/step - loss: 2.7883\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 1399s 8s/step - loss: 2.0877\n",
            "Epoch 3/50\n",
            " 73/172 [===========>..................] - ETA: 13:36 - loss: 1.8807"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPSto3uimSKp"
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZIEZWE4mNKl"
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ_5p0ehKFDn"
      },
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPSALdQXA3l3"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAJqhD9AA5mF"
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}